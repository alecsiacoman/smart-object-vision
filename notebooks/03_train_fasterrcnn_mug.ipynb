{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & paths\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / \"mug_coco_yolo\"\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"images\" / \"train2017\"\n",
    "TRAIN_LABELS_DIR = DATA_ROOT / \"labels\" / \"train2017\"\n",
    "VAL_IMAGES_DIR = DATA_ROOT / \"images\" / \"val2017\"\n",
    "VAL_LABELS_DIR = DATA_ROOT / \"labels\" / \"val2017\"\n",
    "\n",
    "print(\"Train images:\", TRAIN_IMAGES_DIR)\n",
    "print(\"Train labels:\", TRAIN_LABELS_DIR)\n",
    "print(\"Val images:\", VAL_IMAGES_DIR)\n",
    "print(\"Val labels:\", VAL_LABELS_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 2 — Dataset class for YOLO-format labels\n",
    "\n",
    "class MugYoloDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads images + YOLO-format labels and returns data for Faster R-CNN:\n",
    "      image: Tensor [3, H, W]\n",
    "      target: dict with:\n",
    "          - boxes: FloatTensor [N, 4] (x1, y1, x2, y2 in pixels)\n",
    "          - labels: LongTensor [N] (1 for mug)\n",
    "          - image_id: Tensor [1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir: Path, labels_dir: Path, transforms=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_paths = sorted(list(self.images_dir.glob(\"*.jpg\")))\n",
    "        print(f\"Found {len(self.image_paths)} images in {images_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _load_yolo_labels(self, label_path: Path, img_w: int, img_h: int):\n",
    "        boxes = []\n",
    "        if not label_path.exists():\n",
    "            return boxes\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls, xc, yc, w, h = parts\n",
    "                xc = float(xc) * img_w\n",
    "                yc = float(yc) * img_h\n",
    "                w = float(w) * img_w\n",
    "                h = float(h) * img_h\n",
    "\n",
    "                x1 = xc - w / 2\n",
    "                y1 = yc - h / 2\n",
    "                x2 = xc + w / 2\n",
    "                y2 = yc + h / 2\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "        return boxes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = self.labels_dir / (img_path.stem + \".txt\")\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Could not read image: {img_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w = img.shape[:2]\n",
    "\n",
    "        boxes = self._load_yolo_labels(label_path, img_w, img_h)\n",
    "\n",
    "        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        if len(boxes) == 0:\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            labels_tensor = torch.ones((len(boxes),), dtype=torch.int64)  # 1 = mug\n",
    "\n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "            \"image_id\": image_id,\n",
    "        }\n",
    "\n",
    "        img_tensor = transforms.ToTensor()(img)  # [0,1] float\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "\n",
    "        return img_tensor, target\n"
   ],
   "id": "3cf080439b687966"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 3 — Collate function for variable-size targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    return list(images), list(targets)\n"
   ],
   "id": "3d637e7a6bb8e7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 4 — Create datasets and dataloaders\n",
    "\n",
    "train_dataset = MugYoloDataset(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, transforms=None)\n",
    "val_dataset   = MugYoloDataset(VAL_IMAGES_DIR,   VAL_LABELS_DIR,   transforms=None)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ],
   "id": "e62eb08e9d5cb51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 5 — Visual sanity check: show one sample\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "print(\"Batch size:\", len(images))\n",
    "print(\"Image shape:\", images[0].shape)\n",
    "print(\"Target keys:\", targets[0].keys())\n",
    "print(\"Boxes:\", targets[0][\"boxes\"])\n",
    "print(\"Labels:\", targets[0][\"labels\"])\n",
    "\n",
    "# plot the first image with boxes\n",
    "img = images[0].permute(1, 2, 0).numpy()  # [H, W, C]\n",
    "img_h, img_w = img.shape[:2]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(img)\n",
    "for box in targets[0][\"boxes\"]:\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                             linewidth=2, edgecolor='g', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "id": "be0d9c7de917fc7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 6 — Define the Faster R-CNN model (pretrained on COCO, fine-tune on mug)\n",
    "\n",
    "num_classes = 2  # background + mug\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"COCO_V1\"\n",
    ")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n"
   ],
   "id": "352c1b2a2d1101a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 7 — Training loop (simple version)\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "LR = 0.005\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=LR, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, targets in tqdm(data_loader, desc=f\"Epoch {epoch}\"):\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    avg_loss = train_one_epoch(model, train_loader, optimizer, DEVICE, epoch)\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} - Avg Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "a0e59dd12d2a1f76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 8 — Save trained Faster R-CNN weights\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "faster_rcnn_path = OUTPUT_DIR / \"fasterrcnn_mug.pth\"\n",
    "torch.save(model.state_dict(), faster_rcnn_path)\n",
    "\n",
    "print(\"Saved Faster R-CNN weights to:\", faster_rcnn_path)\n"
   ],
   "id": "fa7bb808c5604d60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 9 — Quick inference test on one validation image\n",
    "\n",
    "model.eval()\n",
    "imgs, targets = next(iter(val_loader))\n",
    "imgs = [img.to(DEVICE) for img in imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(imgs)\n",
    "\n",
    "print(\"Predicted boxes:\", outputs[0][\"boxes\"])\n",
    "print(\"Scores:\", outputs[0][\"scores\"])\n",
    "print(\"Labels:\", outputs[0][\"labels\"])\n"
   ],
   "id": "12747674cceddd84"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

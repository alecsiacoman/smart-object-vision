{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & paths\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / \"mug_coco_yolo\"\n",
    "VAL_IMAGES_DIR = DATA_ROOT / \"images\" / \"val2017\"\n",
    "VAL_LABELS_DIR = DATA_ROOT / \"labels\" / \"val2017\"\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "print(\"Val images:\", VAL_IMAGES_DIR)\n",
    "print(\"Val labels:\", VAL_LABELS_DIR)\n",
    "print(\"Outputs:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 2 — Helper: load YOLO-format ground truth labels\n",
    "\n",
    "def load_yolo_labels(label_path, img_w, img_h):\n",
    "    boxes = []\n",
    "    if not label_path.exists():\n",
    "        return boxes\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            cls, xc, yc, w, h = parts\n",
    "            cls = int(cls)\n",
    "            # For mug-only dataset, cls should be 0\n",
    "            xc = float(xc) * img_w\n",
    "            yc = float(yc) * img_h\n",
    "            w = float(w) * img_w\n",
    "            h = float(h) * img_h\n",
    "            x1 = xc - w / 2\n",
    "            y1 = yc - h / 2\n",
    "            x2 = xc + w / 2\n",
    "            y2 = yc + h / 2\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    return np.array(boxes, dtype=np.float32)\n"
   ],
   "id": "cbb5459eebeda440"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 3 — IoU computation (pairwise)\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box1, box2: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_w = max(0, x2 - x1)\n",
    "    inter_h = max(0, y2 - y1)\n",
    "    inter = inter_w * inter_h\n",
    "\n",
    "    area1 = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    area2 = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "\n",
    "    union = area1 + area2 - inter\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n"
   ],
   "id": "62b7698a668f0046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 4 — Helper: compute AP & AR from predictions and ground truths\n",
    "\n",
    "def evaluate_predictions(preds, gts, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    preds: list of dicts:\n",
    "        {\n",
    "          'image_id': str,\n",
    "          'boxes': np.array [N_pred, 4],\n",
    "          'scores': np.array [N_pred]\n",
    "        }\n",
    "\n",
    "    gts: dict mapping image_id -> np.array [N_gt, 4]\n",
    "\n",
    "    Returns:\n",
    "      - AP (Average Precision)\n",
    "      - AR (Average Recall)\n",
    "    \"\"\"\n",
    "\n",
    "    all_scores = []\n",
    "    all_matches = []\n",
    "    num_gt_total = 0\n",
    "\n",
    "    for image_id, gt_boxes in gts.items():\n",
    "        num_gt_total += len(gt_boxes)\n",
    "\n",
    "    # Flatten predictions with image_id\n",
    "    flat_preds = []\n",
    "    for p in preds:\n",
    "        img_id = p[\"image_id\"]\n",
    "        boxes = p[\"boxes\"]\n",
    "        scores = p[\"scores\"]\n",
    "        for b, s in zip(boxes, scores):\n",
    "            flat_preds.append((img_id, b, s))\n",
    "\n",
    "    # Sort by score descending\n",
    "    flat_preds.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # For each GT box, track whether it was already \"matched\"\n",
    "    gt_matched = {\n",
    "        img_id: np.zeros(len(gts[img_id]), dtype=bool)\n",
    "        for img_id in gts.keys()\n",
    "    }\n",
    "\n",
    "    tp = []\n",
    "    fp = []\n",
    "\n",
    "    for img_id, box_pred, score in flat_preds:\n",
    "        all_scores.append(score)\n",
    "\n",
    "        if img_id not in gts or len(gts[img_id]) == 0:\n",
    "            # prediction on an image with no cups -> false positive\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "            continue\n",
    "\n",
    "        gt_boxes = gts[img_id]\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "\n",
    "        for i, box_gt in enumerate(gt_boxes):\n",
    "            iou = compute_iou(box_pred, box_gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = i\n",
    "\n",
    "        if best_iou >= iou_threshold and not gt_matched[img_id][best_gt_idx]:\n",
    "            # True positive\n",
    "            tp.append(1)\n",
    "            fp.append(0)\n",
    "            gt_matched[img_id][best_gt_idx] = True\n",
    "        else:\n",
    "            # False positive\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "\n",
    "    if len(tp) == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    tp = np.array(tp)\n",
    "    fp = np.array(fp)\n",
    "\n",
    "    cum_tp = np.cumsum(tp)\n",
    "    cum_fp = np.cumsum(fp)\n",
    "\n",
    "    recalls = cum_tp / max(num_gt_total, 1)\n",
    "    precisions = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n",
    "\n",
    "    # Average Precision as area under P-R curve (simple numeric integration)\n",
    "    # We'll do a standard 11-point approximation or trapezoidal: here trapezoidal.\n",
    "    ap = 0.0\n",
    "    if len(recalls) > 1:\n",
    "        # sort by recall ascending\n",
    "        order = np.argsort(recalls)\n",
    "        r_sorted = recalls[order]\n",
    "        p_sorted = precisions[order]\n",
    "        for i in range(1, len(r_sorted)):\n",
    "            dr = r_sorted[i] - r_sorted[i-1]\n",
    "            ap += p_sorted[i] * dr\n",
    "\n",
    "    # Average Recall as mean of recall values where there is at least one prediction\n",
    "    ar = recalls[-1] if len(recalls) > 0 else 0.0\n",
    "\n",
    "    return float(ap), float(ar)\n"
   ],
   "id": "c737f1ae33952707"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 5 — Load YOLOv8 model\n",
    "\n",
    "# Adjust path if you trained with a different name\n",
    "YOLO_WEIGHTS = OUTPUT_DIR / \"mug_yolov8n_notebook\" / \"weights\" / \"best.pt\"\n",
    "print(\"YOLO weights:\", YOLO_WEIGHTS, \" | Exists:\", YOLO_WEIGHTS.exists())\n",
    "\n",
    "yolo_model = YOLO(str(YOLO_WEIGHTS))\n"
   ],
   "id": "ea77bc0fbe156b36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 6 — Load Faster R-CNN model\n",
    "\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "num_classes = 2  # background + mug\n",
    "\n",
    "fasterrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "in_features = fasterrcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "fasterrcnn.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "faster_rcnn_path = OUTPUT_DIR / \"fasterrcnn_mug.pth\"\n",
    "print(\"Faster R-CNN weights:\", faster_rcnn_path, \" | Exists:\", faster_rcnn_path.exists())\n",
    "\n",
    "state = torch.load(faster_rcnn_path, map_location=DEVICE)\n",
    "fasterrcnn.load_state_dict(state)\n",
    "fasterrcnn.to(DEVICE)\n",
    "fasterrcnn.eval()\n"
   ],
   "id": "f2bb92f552b2d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 7 — Collect ground truth boxes for all val images\n",
    "\n",
    "val_image_paths = sorted(list(VAL_IMAGES_DIR.glob(\"*.jpg\")))\n",
    "print(\"Num val images:\", len(val_image_paths))\n",
    "\n",
    "gt_boxes_by_image = {}\n",
    "\n",
    "for img_path in val_image_paths:\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    label_path = VAL_LABELS_DIR / (img_path.stem + \".txt\")\n",
    "    gt_boxes = load_yolo_labels(label_path, w, h)  # [N_gt, 4] or []\n",
    "    gt_boxes_by_image[img_path.name] = gt_boxes\n"
   ],
   "id": "e2680a990ccda235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 8 — Run YOLOv8 on all validation images and collect predictions\n",
    "\n",
    "yolo_preds = []\n",
    "conf_threshold = 0.25\n",
    "\n",
    "for img_path in tqdm(val_image_paths, desc=\"YOLOv8 val inference\"):\n",
    "    results = yolo_model.predict(\n",
    "        source=str(img_path),\n",
    "        imgsz=640,\n",
    "        conf=conf_threshold,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    r = results[0]\n",
    "    if r.boxes is None or len(r.boxes) == 0:\n",
    "        boxes_xyxy = np.zeros((0, 4), dtype=np.float32)\n",
    "        scores = np.zeros((0,), dtype=np.float32)\n",
    "    else:\n",
    "        boxes_xyxy = r.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
    "        scores = r.boxes.conf.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    yolo_preds.append({\n",
    "        \"image_id\": img_path.name,\n",
    "        \"boxes\": boxes_xyxy,\n",
    "        \"scores\": scores,\n",
    "    })\n"
   ],
   "id": "890822205228884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 9 — Run Faster R-CNN on all validation images and collect predictions\n",
    "\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "faster_preds = []\n",
    "\n",
    "for img_path in tqdm(val_image_paths, desc=\"Faster R-CNN val inference\"):\n",
    "    img_bgr = cv2.imread(str(img_path))\n",
    "    if img_bgr is None:\n",
    "        boxes_xyxy = np.zeros((0, 4), dtype=np.float32)\n",
    "        scores = np.zeros((0,), dtype=np.float32)\n",
    "    else:\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = to_tensor(img_rgb).to(DEVICE).unsqueeze(0)  # [1,3,H,W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = fasterrcnn(img_tensor)\n",
    "\n",
    "        out = outputs[0]\n",
    "        boxes_xyxy = out[\"boxes\"].detach().cpu().numpy().astype(np.float32)\n",
    "        scores = out[\"scores\"].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # optional: apply score threshold\n",
    "        keep = scores >= conf_threshold\n",
    "        boxes_xyxy = boxes_xyxy[keep]\n",
    "        scores = scores[keep]\n",
    "\n",
    "    faster_preds.append({\n",
    "        \"image_id\": img_path.name,\n",
    "        \"boxes\": boxes_xyxy,\n",
    "        \"scores\": scores,\n",
    "    })\n"
   ],
   "id": "cf0800edb6dc3b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 10 — Evaluate YOLOv8 (IoU@0.5, AP, AR)\n",
    "\n",
    "ap_yolo, ar_yolo = evaluate_predictions(yolo_preds, gt_boxes_by_image, iou_threshold=0.5)\n",
    "\n",
    "print(f\"YOLOv8 @ IoU=0.5 -> AP: {ap_yolo:.4f}, AR: {ar_yolo:.4f}\")\n"
   ],
   "id": "23fc24ac79a339b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 11 — Evaluate Faster R-CNN (IoU@0.5, AP, AR)\n",
    "\n",
    "ap_frcnn, ar_frcnn = evaluate_predictions(faster_preds, gt_boxes_by_image, iou_threshold=0.5)\n",
    "\n",
    "print(f\"Faster R-CNN @ IoU=0.5 -> AP: {ap_frcnn:.4f}, AR: {ar_frcnn:.4f}\")\n"
   ],
   "id": "806fddb34de8a60f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 12 — Summary comparison\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"model\": \"YOLOv8\",       \"IoU@0.5\": 0.5, \"AP\": ap_yolo,   \"AR\": ar_yolo},\n",
    "    {\"model\": \"Faster R-CNN\", \"IoU@0.5\": 0.5, \"AP\": ap_frcnn, \"AR\": ar_frcnn},\n",
    "])\n",
    "\n",
    "df\n"
   ],
   "id": "89dc3ac22f86e935"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
